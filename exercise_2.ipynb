{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import os\n",
    "import gzip\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second exercise: Classifying MNIST with Tensorflow Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "We first define a function for downloading and loading MNIST.\n",
    "**WARNING**: Executing it will obviously use up some space on your machine ;). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist(datasets_dir='./data'):\n",
    "    if not os.path.exists(datasets_dir):\n",
    "        os.mkdir(datasets_dir)\n",
    "    data_file = os.path.join(datasets_dir, 'mnist.pkl.gz')\n",
    "    if not os.path.exists(data_file):\n",
    "        print('... downloading MNIST from the web')\n",
    "        try:\n",
    "            import urllib\n",
    "            urllib.urlretrieve('http://google.com')\n",
    "        except AttributeError:\n",
    "            import urllib.request as urllib\n",
    "        url = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        urllib.urlretrieve(url, data_file)\n",
    "\n",
    "    print('... loading data')\n",
    "    # Load the dataset\n",
    "    f = gzip.open(data_file, 'rb')\n",
    "    try:\n",
    "        train_set, valid_set, test_set = cPickle.load(f, encoding=\"latin1\")\n",
    "    except TypeError:\n",
    "        train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    test_x, test_y = test_set\n",
    "    test_x = test_x.astype('float32')\n",
    "    test_x = test_x.astype('float32').reshape(test_x.shape[0], 1, 28, 28)\n",
    "    test_y = test_y.astype('int32')\n",
    "    valid_x, valid_y = valid_set\n",
    "    valid_x = valid_x.astype('float32')\n",
    "    valid_x = valid_x.astype('float32').reshape(valid_x.shape[0], 1, 28, 28)\n",
    "    valid_y = valid_y.astype('int32')\n",
    "    train_x, train_y = train_set\n",
    "    train_x = train_x.astype('float32').reshape(train_x.shape[0], 1, 28, 28)\n",
    "    train_y = train_y.astype('int32')\n",
    "    rval = [(train_x, train_y), (valid_x, valid_y), (test_x, test_y)]\n",
    "    print('... done loading data')\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build up the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal was it to build up a cnn with tensorflow and observe what some parameters/configurations change.\n",
    "\n",
    "Most of the code could be copied from the tensorflow tutorial. Only minor changes were needed.\n",
    "Source: https://www.tensorflow.org/get_started/mnist/pros\n",
    "\n",
    "What changed in comparison to the tutorial online:\n",
    "    1. the datasource was changed to the mnist set from the previous exercise\n",
    "    2. the filter sizes were adjusted\n",
    "    3. removed drop out (as it's not required in the exercise)\n",
    "    4. added variable filter size   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "def deepnn(x, no_filters):\n",
    "  \"\"\"deepnn builds the graph for a deep net for classifying digits.\n",
    "  Args:\n",
    "    x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n",
    "    number of pixels in a standard MNIST image.\n",
    "  Returns:\n",
    "    A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n",
    "    equal to the logits of classifying the digit into one of 10 classes (the\n",
    "    digits 0-9).\n",
    "  \"\"\"\n",
    "  # Reshape to use within a convolutional neural net.\n",
    "  # Last dimension is for \"features\" - there is only one here, since images are\n",
    "  # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "  with tf.name_scope('reshape'):\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "  # First convolutional layer\n",
    "  with tf.name_scope('conv1'):\n",
    "    W_conv1 = weight_variable([3, 3, 1, no_filters])\n",
    "    b_conv1 = bias_variable([no_filters])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "  # Pooling layer\n",
    "  with tf.name_scope('pool1'):\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "\n",
    "  # Second convolutional layer\n",
    "  with tf.name_scope('conv2'):\n",
    "    W_conv2 = weight_variable([3, 3, no_filters, no_filters])\n",
    "    b_conv2 = bias_variable([no_filters])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    \n",
    "  # Second pooling layer\n",
    "  with tf.name_scope('pool2'):\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    print(h_pool2.shape)\n",
    "\n",
    "\n",
    "  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "  # is down to 7x7x(no_filters) feature maps -- maps this to 128 features.\n",
    "  with tf.name_scope('fc1'):\n",
    "    W_fc1 = weight_variable([7 * 7 * no_filters, 128])\n",
    "    b_fc1 = bias_variable([128])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * no_filters])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "  # Map the 128 features to 10 classes, one for each digit\n",
    "  with tf.name_scope('fc2'):\n",
    "    W_fc2 = weight_variable([128, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "    y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "  return y_conv\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def one_hot(labels):\n",
    "  \"\"\"this creates a one hot encoding from a flat vector:\n",
    "  i.e. given y = [0,2,1]\n",
    "  it creates y_one_hot = [[1,0,0], [0,0,1], [0,1,0]]\n",
    "  \"\"\"\n",
    "  classes = np.unique(labels)\n",
    "  n_classes = classes.size\n",
    "  one_hot_labels = np.zeros(labels.shape + (n_classes,))\n",
    "  for c in classes:\n",
    "      one_hot_labels[labels == c, c] = 1\n",
    "  return one_hot_labels\n",
    "\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  Dtrain, Dval, Dtest = mnist()\n",
    "  X_train, y_train = Dtrain\n",
    "  X_test, y_test = Dtest \n",
    "  x_val, y_val = Dval\n",
    "    \n",
    "  X_train2 = X_train[:,0,:,:]\n",
    "  X_training = X_train2.reshape(50000,784)\n",
    "  Y_training = one_hot(y_train)\n",
    "\n",
    "  x_val2 = x_val[:,0,:,:]\n",
    "  X_validation = x_val2.reshape(10000,784)\n",
    "  Y_validation = one_hot(y_val)\n",
    "  print(X_training.shape)\n",
    "    \n",
    "  \n",
    "  hyperparams_learning_rate = [0.1, 0.01, 0.001, 0.0001]\n",
    "  hyperparams_no_filters = [8, 16, 32, 64, 128, 256]\n",
    "  for hyp_l_r in range(4):       \n",
    "    # Create the model\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    # Define loss and optimizer\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    # Build the graph for the deep net\n",
    "    y_conv = deepnn(x, hyperparams_no_filters[1])\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "      cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,\n",
    "                                                            logits=y_conv)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "        \n",
    "    with tf.name_scope('stochastic_gradient_optimizer'):\n",
    "      train_step = tf.train.GradientDescentOptimizer(hyperparams_learning_rate[hyp_l_r]).minimize(cross_entropy)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "      correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "      correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "    graph_location = tempfile.mkdtemp()\n",
    "    print('Saving graph to: %s' % graph_location)\n",
    "    train_writer = tf.summary.FileWriter(graph_location)\n",
    "    train_writer.add_graph(tf.get_default_graph())\n",
    "    config = tf.ConfigProto(\n",
    "       device_count = {'GPU': 0}\n",
    "    )\n",
    "    \n",
    "    path = '/home/johannes/file'+str(hyperparams_learning_rate[hyp_l_r])+'.txt'\n",
    "    learning = open(path,'w')\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "      sess.run(tf.global_variables_initializer())\n",
    "      for i in range(7500):        \n",
    "        validation_accuracy = accuracy.eval(feed_dict={\n",
    "        x: X_validation[0:10000,:], y_: Y_validation[0:10000,:]})\n",
    "        t1 = time.time() \n",
    "        for ii in range(10):\n",
    "            train_step.run(feed_dict={x: X_training[5000*ii:5000*(ii+1),:], y_: Y_training[5000*ii:5000*(ii+1),:]})\n",
    "        t2 = time.time()        \n",
    "        print('step %d, validation accuracy %g' % (i, validation_accuracy))\n",
    "        print(t2-t1)\n",
    "        learning.write(str(validation_accuracy)+'\\n')\n",
    "      learning.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--data_dir', type=str,\n",
    "                      default='/tmp/tensorflow/mnist/input_data',\n",
    "                      help='Directory for storing input data')\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the exercise 2.3 I just copied the code above and made little changes to measure the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  Dtrain, Dval, Dtest = mnist()\n",
    "  X_train, y_train = Dtrain\n",
    "  X_test, y_test = Dtest \n",
    "  x_val, y_val = Dval\n",
    "    \n",
    "  X_train2 = X_train[:,0,:,:]\n",
    "  X_training = X_train2.reshape(50000,784)\n",
    "  Y_training = one_hot(y_train)\n",
    "\n",
    "  x_val2 = x_val[:,0,:,:]\n",
    "  X_validation = x_val2.reshape(10000,784)\n",
    "  Y_validation = one_hot(y_val)\n",
    "  print(X_training.shape)\n",
    "    \n",
    "  hyperparams_no_filters = [8, 16, 32, 64, 128, 256]\n",
    "  for hyp_l_r in range(6):\n",
    "    #for hyp_n_f in range(6):\n",
    "    \n",
    "    # Create the model\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # Build the graph for the deep net\n",
    "    y_conv = deepnn(x, hyperparams_no_filters[hyp_l_r])\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "      cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,\n",
    "                                                            logits=y_conv)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "        \n",
    "    with tf.name_scope('stochastic_gradient_optimizer'):\n",
    "      train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "      correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "      correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "    graph_location = tempfile.mkdtemp()\n",
    "    print('Saving graph to: %s' % graph_location)\n",
    "    train_writer = tf.summary.FileWriter(graph_location)\n",
    "    train_writer.add_graph(tf.get_default_graph())\n",
    "    config = tf.ConfigProto(\n",
    "       device_count = {'GPU': 0}\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "      sess.run(tf.global_variables_initializer())        \n",
    "      t1 = time.time() \n",
    "      # to measure the runtime its not necessary to run whole epochs. I just take 500 steps to have reduce the python \n",
    "      # overhed of the calculations\n",
    "      for i in range(500):\n",
    "        ii = 1\n",
    "        train_step.run(feed_dict={x: X_training[500*ii:500*(ii+1),:], y_: Y_training[500*ii:500*(ii+1),:]})\n",
    "      t2 = time.time()        \n",
    "      print(t2-t1)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--data_dir', type=str,\n",
    "                      default='/tmp/tensorflow/mnist/input_data',\n",
    "                      help='Directory for storing input data')\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot gerneration (only for completion)\n",
    "The data have bin copied by hand or stored in text files. The actual plots can be found in the pdf.\n",
    "\n",
    "Plot 1 (Learning rates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "hyperparams_learning_rate = [0.1, 0.01, 0.001, 0.0001]\n",
    "data = []\n",
    "\n",
    "for hyp_l_r in range(4):\n",
    "    path = '/home/johannes/file' + str(hyperparams_learning_rate[hyp_l_r]) + '.txt'\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "    content = [float(x.strip())*100 for x in content]\n",
    "    data.append(content)\n",
    "\n",
    "x = list(range(0, len(content)))\n",
    "\n",
    "plt.plot(x, data[0], label=\"0.1\")\n",
    "plt.plot(x, data[1], label=\"0.01\")\n",
    "plt.plot(x, data[2], label=\"0.001\")\n",
    "plt.plot(x, data[3], label=\"0.0001\")\n",
    "plt.axis([0, 7500,0,100])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Error in %')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0., title='Learning rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot 2 (Scatter GPU/CPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "GPU_tuple = [(4.2710609436, 8) , (5.86049294472, 16) , (8.88112902641, 32) , (17.1756880283, 64), (34.8885371685, 128), (77.2009470463, 256)]\n",
    "CPU_tuple = [(43.9258110523, 8), (68.2844769955, 16), (173.806190968, 32), (446.362864017,64)]\n",
    "\n",
    "x = [8, 16, 32, 64, 128, 256]\n",
    "x2 = [8, 16, 32, 64]\n",
    "y = [4.2710609436 , 5.86049294472, 8.88112902641, 17.1756880283, 34.8885371685, 77.2009470463]\n",
    "y2 = [43.9258110523, 68.2844769955, 173.806190968, 446.362864017]\n",
    "color = ['r','r','r','r','r','r','b','b','b','b']\n",
    "sc1 = plt.scatter(x, y, c='r', alpha=0.5)\n",
    "sc2 = plt.scatter(x2, y2, c='b', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Filter')\n",
    "plt.ylabel('Ausf√ºhrungszeit in Sekunden')\n",
    "\n",
    "plt.legend((sc1, sc2),('GPU','CPU'), title='Berechnet auf:')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
